{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network by Hand\n",
        "## CPE 490 590\n",
        "## Rahul Bhadani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider a sample dataset with two features, three training samples $\\mathbf{x} = [ x_1^{(1)}, x_2^{(1)} ], [ x_1^{(2)}, x_2^{(2)} ], [ x_1^{(3)}, x_2^{(3)} ] = [1, 4], [5, 6], [9, 12] $ and response variable $\\mathbf{y} = y^{(1)}, y^{(2)}, y^{(3)} = [-1, 0, 1]$.\n",
        "\n",
        "We want to build a neural networl containing two hidden layer: the first hidden layer will contain 4 hidden units or neurons, the second hidden layer will contain two hidden units or neurons. The prediction will be about predicting class value out of -1, 0, and 1. We will use cross entropy loss and optimization algorithm is stochastic gradient descent with learning rate of 0.05. The activation function to be used is sigmoid function.\n",
        "\n",
        "The following code runs trains the neural network for two epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRS06ev93qOg",
        "outputId": "574f1195-1a44-4188-e584-d3543b602dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "\n",
            " Iteration: 0\n",
            "\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "\n",
            " Forward Propagation\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "Output from first hidden layer:\n",
            " [[0.86989153 0.96442881 0.76852478 0.98901306]\n",
            " [0.97068777 0.99726804 0.96770454 0.99993872]\n",
            " [0.9987706  0.99998763 0.99752738 0.99999999]]\n",
            "Output from second hidden layer:\n",
            " [[0.94406221 0.87537515]\n",
            " [0.95510906 0.88793552]\n",
            " [0.9567904  0.89077129]]\n",
            "Output from output layer:\n",
            " [[0.81883046]\n",
            " [0.8204345 ]\n",
            " [0.82075924]]\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "\n",
            " Backward Propagation\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "Error:\n",
            " 5.1315064296247845\n",
            "Derivative of the error with respect to Wo:\n",
            " [[ 1.81883046]\n",
            " [ 0.8204345 ]\n",
            " [-0.17924076]]\n",
            "Derivative of the error with respect to Wh2:\n",
            " [[ 0.02881505  0.11905354]\n",
            " [ 0.01055302  0.04898291]\n",
            " [-0.00222308 -0.01046384]]\n",
            "Derivative of the error with respect to Wh1:\n",
            " [[ 1.40838368e-02  1.60766787e-03  5.70612263e-03  5.44654432e-04]\n",
            " [ 1.43449994e-03  4.96921004e-05  3.83949244e-04  1.22376619e-06]\n",
            " [-1.32013869e-05 -4.78972716e-08 -6.41918876e-06 -5.85773142e-11]]\n",
            "Updated weights and biases after backpropagation:\n",
            "Wo:\n",
            " [[0.18354012]\n",
            " [0.49195053]]\n",
            "Bo:\n",
            " [0.57699879]\n",
            "Wh2:\n",
            " [[0.59834553 0.89296701]\n",
            " [0.79819544 0.1923398 ]\n",
            " [0.69849301 0.09357707]\n",
            " [0.49815861 0.29218692]]\n",
            "Bh2:\n",
            " [0.49026412 0.59026412]\n",
            "Wh1:\n",
            " [[0.19894312 0.29990722 0.4996216  0.89997246]\n",
            " [0.3967608  0.69966359 0.09874744 0.7998907 ]]\n",
            "Bh1:\n",
            " [0.0988104 0.1988104 0.2988104 0.3988104]\n",
            "\n",
            "============================================================\n",
            "\n",
            " Iteration: 1\n",
            "\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "\n",
            " Forward Propagation\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "Output from first hidden layer:\n",
            " [[0.86816112 0.96433854 0.76735249 0.98899507]\n",
            " [0.96994152 0.99725802 0.96737171 0.9999386 ]\n",
            " [0.9987082  0.99998755 0.99747849 0.99999999]]\n",
            "Output from second hidden layer:\n",
            " [[0.94311314 0.87121698]\n",
            " [0.95436893 0.88400289]\n",
            " [0.95609851 0.88694846]]\n",
            "Output from output layer:\n",
            " [[0.76471282]\n",
            " [0.76621295]\n",
            " [0.76652924]]\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "\n",
            " Backward Propagation\n",
            "\n",
            "____________________________________________________________\n",
            "\n",
            "Error:\n",
            " 4.344869045818073\n",
            "Derivative of the error with respect to Wo:\n",
            " [[ 1.76471282]\n",
            " [ 0.76621295]\n",
            " [-0.23347076]]\n",
            "Derivative of the error with respect to Wh2:\n",
            " [[ 0.01737724  0.09740481]\n",
            " [ 0.00612431  0.03865198]\n",
            " [-0.00179864 -0.01151672]]\n",
            "Derivative of the error with respect to Wh1:\n",
            " [[ 1.11455035e-02  1.12128553e-03  3.79409509e-03  4.03975307e-04]\n",
            " [ 1.11311930e-03  3.36960278e-05  2.49186769e-04  8.80716905e-07]\n",
            " [-1.46561985e-05 -4.54445046e-08 -5.87046179e-06 -5.88816423e-11]]\n",
            "Updated weights and biases after backpropagation:\n",
            "Wo:\n",
            " [[0.07492248]\n",
            " [0.39156525]]\n",
            "Bo:\n",
            " [0.46212604]\n",
            "Wh2:\n",
            " [[0.59738403 0.88743944]\n",
            " [0.79714212 0.18629177]\n",
            " [0.69761977 0.08854472]\n",
            " [0.49708305 0.28601363]]\n",
            "Bh2:\n",
            " [0.48295197 0.58295197]\n",
            "Wh1:\n",
            " [[0.19811416 0.29984275 0.49937224 0.89995204]\n",
            " [0.39420656 0.69942925 0.09791739 0.79980964]]\n",
            "Bh1:\n",
            " [0.09791834 0.19791834 0.29791834 0.39791834]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Cross-Entropy loss and its derivative\n",
        "def cross_entropy_loss(y_actual, y_pred):\n",
        "    return -np.sum(np.multiply(y_actual, np.log(y_pred)) + np.multiply((1 - y_actual), np.log(1 - y_pred)))\n",
        "\n",
        "def cross_entropy_loss_derivative(y_actual, y_pred):\n",
        "    return -(np.divide(y_actual, y_pred) - np.divide(1 - y_actual, 1 - y_pred))\n",
        "\n",
        "# Input samples\n",
        "X = np.array([[1, 4], [5, 6], [9, 12]])\n",
        "\n",
        "# Actual output\n",
        "Y = np.array([[-1], [0], [1]])\n",
        "\n",
        "# Weights and biases initialization from previous example\n",
        "weights_hidden1 = np.array([[0.2, 0.3, 0.5, 0.9], [0.4, 0.7, 0.1, 0.8]])\n",
        "biases_hidden1 = np.array([0.1, 0.2, 0.3, 0.4])\n",
        "weights_hidden2 = np.array([[0.6, 0.9], [0.8, 0.2], [0.7, 0.1], [0.5, 0.3]])\n",
        "biases_hidden2 = np.array([0.5, 0.6])\n",
        "weights_output = np.array([[0.3], [0.6]])\n",
        "biases_output = np.array([0.7])\n",
        "\n",
        "# Learning rate\n",
        "eta = 0.05\n",
        "\n",
        "\n",
        "# Training for 2 iterations\n",
        "for iii in range(2):\n",
        "    # Forward Propagation\n",
        "    print(\"\\n============================================================\")\n",
        "    print(\"\\n Iteration: {}\\n\".format(iii))\n",
        "    print(\"\\n____________________________________________________________\\n\")\n",
        "    print(\"\\n Forward Propagation\")\n",
        "    print(\"\\n____________________________________________________________\\n\")\n",
        "\n",
        "    hidden_layer1_output = sigmoid(np.dot(X, weights_hidden1) + biases_hidden1)\n",
        "    print(\"Output from first hidden layer:\\n\", hidden_layer1_output)\n",
        "\n",
        "    hidden_layer2_output = sigmoid(np.dot(hidden_layer1_output, weights_hidden2) + biases_hidden2)\n",
        "    print(\"Output from second hidden layer:\\n\", hidden_layer2_output)\n",
        "\n",
        "    predicted_output = sigmoid(np.dot(hidden_layer2_output, weights_output) + biases_output)\n",
        "    print(\"Output from output layer:\\n\", predicted_output)\n",
        "\n",
        "    print(\"\\n____________________________________________________________\\n\")\n",
        "\n",
        "    print(\"\\n Backward Propagation\")\n",
        "    print(\"\\n____________________________________________________________\\n\")\n",
        "\n",
        "    # Backward Propagation\n",
        "    error = cross_entropy_loss(Y, predicted_output)\n",
        "    print(\"Error:\\n\", error)\n",
        "\n",
        "    d_predicted_output = cross_entropy_loss_derivative(Y, predicted_output) * sigmoid_derivative(predicted_output)\n",
        "    print(\"Derivative of the error with respect to Wo:\\n\", d_predicted_output)\n",
        "\n",
        "    error_hidden_layer2 = d_predicted_output.dot(weights_output.T)\n",
        "    d_hidden_layer2 = error_hidden_layer2 * sigmoid_derivative(hidden_layer2_output)\n",
        "    print(\"Derivative of the error with respect to Wh2:\\n\", d_hidden_layer2)\n",
        "\n",
        "    error_hidden_layer1 = d_hidden_layer2.dot(weights_hidden2.T)\n",
        "    d_hidden_layer1 = error_hidden_layer1 * sigmoid_derivative(hidden_layer1_output)\n",
        "    print(\"Derivative of the error with respect to Wh1:\\n\", d_hidden_layer1)\n",
        "\n",
        "    # Updating Weights and Biases\n",
        "    weights_output -= eta * hidden_layer2_output.T.dot(d_predicted_output)\n",
        "    biases_output -= eta * np.sum(d_predicted_output)\n",
        "\n",
        "    weights_hidden2 -= eta * hidden_layer1_output.T.dot(d_hidden_layer2)\n",
        "    biases_hidden2 -= eta * np.sum(d_hidden_layer2)\n",
        "\n",
        "    weights_hidden1 -= eta * X.T.dot(d_hidden_layer1)\n",
        "    biases_hidden1 -= eta * np.sum(d_hidden_layer1)\n",
        "\n",
        "    print(\"Updated weights and biases after backpropagation:\")\n",
        "    print(\"Wo:\\n\", weights_output)\n",
        "    print(\"Bo:\\n\", biases_output)\n",
        "    print(\"Wh2:\\n\", weights_hidden2)\n",
        "    print(\"Bh2:\\n\", biases_hidden2)\n",
        "    print(\"Wh1:\\n\", weights_hidden1)\n",
        "    print(\"Bh1:\\n\", biases_hidden1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljXTcT5gGIP_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "streamkernel",
      "language": "python",
      "name": "streamkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
